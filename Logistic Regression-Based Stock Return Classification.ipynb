{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc6482e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1>Logistic Regression-Based Stock Return Classification</h1>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9fe22",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In this assignment, we aim to build a classification model that categorizes stock returns into negative, neutral, and positive classes. The model will use daily return signals to predict stock returns over one day, one week, and one month. Our approach involves logistic regression with various features, including price-based, volume-based, technical, fundamental, and macro-economic indicators for 20 stocks across technology, finance, healthcare, and energy sectors. We evaluate the models with metrics such as accuracy, ROC-AUC, and F1-scores to compare their performance. The models will be evaluated and tested across different stocks to determine the most effective prediction horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec592ef",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488adbc",
   "metadata": {},
   "source": [
    "For a classification problem with three possible outcomes, we adopt a multinomial logistic regression model. This model estimates the probability of each class by comparing the exponentiated linear combinations of predictors for each class to the sum of these combinations across all classes. In the formula, $k$ represents a specific class, while $j$ ranges from -1 to 1 in the denominator, ensuring that the probabilities for all classes sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc48c8",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Y_t = k | X_t = x_t) = \\frac{e^{\\alpha_k + \\beta_{k1} \\cdot x_{1,t} + \\beta_{k2} \\cdot x_{2,t} + \\dots + \\beta_{kn} \\cdot x_{n,t}}}{\\sum_{j=-1}^{1} e^{\\alpha_j + \\beta_{j1} \\cdot x_{1,t} + \\beta_{j2} \\cdot x_{2,t} + \\dots + \\beta_{jn} \\cdot x_{n,t}}}\n",
    "$$\n",
    "\n",
    "- $\\beta_i$: Coefficients representing the impact of each feature $X_{i,t}$ on the log-odds of each return class.\n",
    "- $\\alpha$: Intercept term for each class, representing the baseline log-odds when all features are zero.\n",
    "- $Y_t$: Daily return class at time $t$ ($k = -1$ for negative, $0$ for neutral, $1$ for positive), based on next day’s return $R_{t+1} = \\frac{\\text{Price}_{t+1} - \\text{Price}_t}{\\text{Price}_t}$ and threshold $\\tau$.\n",
    "- $P(Y_t = k)$: Probability of the return falling into class $k$.\n",
    "- $X_{n,t}$: Features at time $t$, including the features in the below table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f816f44",
   "metadata": {},
   "source": [
    "| Category                  | Feature                  | Description                                                                 | Daily Model | Weekly Model | Monthly Model |\n",
    "|:--------------------------|:-------------------------|:----------------------------------------------------------------------------|:------------|:-------------|:--------------|\n",
    "| **Price-Based Features**  | Daily Returns            | The percentage change in stock price from one day to the next.              | -           | -            | -             |\n",
    "|                           | Moving Averages          | Simple Moving Average (SMA) and Exponential Moving Average (EMA) over different periods. We chose 5-day, 10-day, and 20-day for daily; 10-day, 20-day, and 50-day for weekly; 20-day, 50-day, and 100-day for monthly SMA and EMA. | X           | X            | X             |\n",
    "|                           | Volatility               | Standard deviation of daily returns over a certain period. We chose 20-day for daily and weekly; 50-day for monthly volatilities.                 | X           | X            | X             |\n",
    "|                           | Relative Strength Index (RSI) | Measures the speed and change of price movements. 14-day is the traditional default for daily, weekly, and monthly momentum.                           | X           | X            | X             |\n",
    "|                           | Bollinger Bands          | Uses moving averages and standard deviations to identify overbought or oversold conditions. We chose 20-day SMA with 2 std devs for daily; 50-day with 2 std devs for weekly; and 100-day with 2 std devs for monthly. | X           | X            | X             |\n",
    "| **Volume-Based Features** | Trading Volume           | The number of shares traded in a given period.                              | X           | X            | X             |\n",
    "|                           | Volume Moving Averages   | Similar to price moving averages but applied to trading volume. 5-day, 10-day, and 20-day for daily; 10-day, 20-day, and 50-day for weekly; 20-day, 50-day, and 100-day for monthly.            | X           | X            | X             |\n",
    "|                           | On-Balance Volume (OBV)  | A cumulative total of volume that adds or subtracts volume based on price movement. | X           | X            | X             |\n",
    "| **Technical Indicators**  | Moving Average Convergence Divergence (MACD) | Shows the relationship between two moving averages of a stock’s price. 12-day EMA, 26-day EMA with a 9-day signal line is the standard setting for daily, weekly, and monthly momentum.      | X           | X            | X             |\n",
    "|                           | Stochastic Oscillator    | Compares a particular closing price of a security to a range of its prices over a certain period. 14-day is the conventional period optimized for daily, weekly, and monthly price range comparisons. | X           | X            | X             |\n",
    "|                           | Average True Range (ATR) | Measures market volatility by decomposing the entire range of an asset price for that period. We chose 14-day for daily and weekly; 50-day for monthly volatility. | X           | X            | X             |\n",
    "| **Fundamental Features**  | Earnings Reports         | Quarterly earnings, earnings per share (EPS), and revenue growth.           |             | X            | X             |\n",
    "|                           | Financial Ratios         | Price-to-Earnings (P/E) ratio, Price-to-Book (P/B) ratio, and Debt-to-Equity ratio. |             | X            | X             |\n",
    "|                           | Dividend Yield           | The dividend income relative to the stock price.                            |             |              | X             |\n",
    "| **Macro-Economic Indicators** | Interest Rates       | Changes in interest rates can affect stock prices.                          |             | X            | X             |\n",
    "|                           | Economic Indicators      | GDP growth rate, unemployment rate, and inflation rate.                     |             |              | X             |\n",
    "| **Time-Based Features**   | Day of the Week          | Some stocks exhibit patterns based on the day of the week.                  | X           |              |               |\n",
    "|                           | Seasonality              | Monthly or quarterly trends. Month of the year for monthly model.           |             | X            | X             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e009a5",
   "metadata": {},
   "source": [
    "## 2.1 Daily Model\n",
    "For the daily model, we aim to predict the next day’s stock return class using short-term price and volume signals in a multinomial logistic regression framework. We selected features like Moving Averages, Volatility, RSI, Bollinger Bands, Trading Volume, Volume Moving Averages, OBV, MACD, Stochastic Oscillator, ATR, and Day of the Week, while excluding the following variables for these reasons:  \n",
    "- **Earnings Reports**: Quarterly data, not daily, making it irrelevant for day-to-day predictions.  \n",
    "- **Financial Ratios**: Updated infrequently (e.g., quarterly), less impactful on daily returns.  \n",
    "- **Dividend Yield**: Static over short periods, more suited for long-term analysis.  \n",
    "- **Interest Rates**: Not daily-updated, better for longer horizons like weekly or monthly.  \n",
    "- **Economic Indicators**: Broad, slow-changing metrics (e.g., GDP), not specific to daily movements.  \n",
    "- **Seasonality**: Monthly/quarterly trends are too coarse; Day of the Week covers daily time effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19c836",
   "metadata": {},
   "source": [
    "## 2.2 Weekly Model\n",
    "For the weekly model, we aim to predict the weekly stock return class using medium-term price, volume, and select fundamental signals in a multinomial logistic regression framework. We selected features like Moving Averages, Volatility, RSI, Bollinger Bands, Trading Volume, Volume Moving Averages, OBV, MACD, Stochastic Oscillator, ATR, Earnings Reports, Financial Ratios, Interest Rates, and Seasonality (Week of the Month), while excluding the following variables for these reasons:\n",
    "\n",
    "- **Dividend Yield**: Static over medium periods, more relevant for long-term analysis.\n",
    "- **Economic Indicators**: Broad, slow-changing metrics (e.g., GDP), better suited for monthly or longer horizons.\n",
    "- **Day of the Week**: Too granular for weekly predictions; Seasonality captures broader time effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b79eaa",
   "metadata": {},
   "source": [
    "## 2.3 Monthly Model\n",
    "For the monthly model, we aim to predict the monthly stock return class using long-term price, volume, fundamental, and macro-economic signals in a multinomial logistic regression framework. We selected features like Moving Averages, Volatility, RSI, Bollinger Bands, Trading Volume, Volume Moving Averages, OBV, MACD, Stochastic Oscillator, ATR, Earnings Reports, Financial Ratios, Dividend Yield, Interest Rates, Economic Indicators, and Seasonality (Month of the Year), while excluding the following variable for this reason:  \n",
    "- **Day of the Week**: Too short-term and granular for monthly predictions; Seasonality captures broader time effects.\n",
    "\n",
    "** <small> Note we excluded daily returns as a feature in above three models because it is closely tied to the target, risking data leakage, and its predictive information is already captured by features like moving averages and momentum indicators such as RSI and MACD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c3fc19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.54)\n",
      "Requirement already satisfied: pandas in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.4-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Requirement already satisfied: ta in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (2.5.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (2.3.10)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Installing collected packages: numpy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "Successfully installed numpy-2.2.4 pandas-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
      "streamlit 1.29.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.4 which is incompatible.\n",
      "scipy 1.10.0 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.4 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 2.2.4 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==2.1.4\n",
      "  Using cached pandas-2.1.4-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "Collecting numpy<2,>=1.22.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.1\n",
      "    Uninstalling pytz-2025.1:\n",
      "      Successfully uninstalled pytz-2025.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.1\n",
      "    Uninstalling tzdata-2025.1:\n",
      "      Successfully uninstalled tzdata-2025.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed numpy-1.26.4 pandas-2.1.4 python-dateutil-2.9.0.post0 pytz-2025.1 six-1.17.0 tzdata-2025.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires python-dateutil==2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fredapi in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from fredapi) (2.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2025.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->fredapi) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\madlin\\appdata\\local\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U yfinance pandas numpy ta\n",
    "!pip install pandas==2.1.4 --force-reinstall\n",
    "!pip install fredapi\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator\n",
    "from ta.volume import OnBalanceVolumeIndicator\n",
    "from ta.trend import MACD\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.momentum import StochasticOscillator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fredapi import Fred\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02073fa2",
   "metadata": {},
   "source": [
    "# 3. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ef2b",
   "metadata": {},
   "source": [
    "## 3.1 Data Source\n",
    "\n",
    "We used the `yfinance` Python library to retrieve daily OHLCV (Open, High, Low, Close, Volume) data. Additionally, the table below provides a high level overview on how we computed th features.\n",
    "\n",
    "| Category                  | Feature                  | General Computation Method                                                                 |\n",
    "|:--------------------------|:-------------------------|:------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Price-Based Features**  | Daily Returns            | - |\n",
    "|                           | Moving Averages          | SMA and EMA calculated using `pandas` `rolling.mean` and `ewm` on `Close`. Periods: 5, 10, 20 (Daily); 10, 20, 50 (Weekly); 20, 50, 100 (Monthly). |\n",
    "|                           | Volatility               | Standard deviation of daily returns from `Close` price changes using `pandas` `rolling.std`. Periods: 20-day (Daily, Weekly); 50-day (Monthly). |\n",
    "|                           | Relative Strength Index (RSI) | 14-day RSI from `Close` using average gains/losses over 14 days via `ta` library or `pandas` (consistent across Daily, Weekly, Monthly). |\n",
    "|                           | Bollinger Bands          | SMA of `Close` ± 2 × standard deviation of `Close` using `pandas` `rolling`. Periods: 20-day SMA, 2 std dev (Daily); 50-day (Weekly); 100-day (Monthly). |\n",
    "| **Volume-Based Features** | Trading Volume           | Direct from `yfinance` `Volume` column, aggregated as sum per period (Daily, Weekly, Monthly).                |\n",
    "|                           | Volume Moving Averages   | SMA of `Volume` using `pandas` `rolling.mean`. Periods: 5, 10, 20 (Daily); 10, 20, 50 (Weekly); 20, 50, 100 (Monthly). |\n",
    "|                           | On-Balance Volume (OBV)  | Cumulative sum of `Volume`, adding if `Close` rises, subtracting if it falls, computed with `pandas` (consistent across horizons). |\n",
    "| **Technical Indicators**  | Moving Average Convergence Divergence (MACD) | 12-day EMA - 26-day EMA of `Close`, with 9-day EMA signal line, using `pandas` `ewm` (consistent across Daily, Weekly, Monthly). |\n",
    "|                           | Stochastic Oscillator    | 14-day %K from `High`, `Low`, `Close`: $\\%K = 100 \\times \\frac{\\text{Close} - \\text{Low}_{14}}{\\text{High}_{14} - \\text{Low}_{14}}$ (consistent across horizons). |\n",
    "|                           | Average True Range (ATR) | 14-day average of true range (max of `High - Low`, adjusted for gaps) using `pandas` for Daily/Weekly; 50-day for Monthly. |\n",
    "| **Fundamental Features**  | Earnings Reports         | Quarterly EPS retrieved from `yfinance` `get_earnings_dates()`, nearest quarter applied and forward-filled within quarter for Weekly/Monthly. |\n",
    "|                           | Financial Ratios         | P/E ratio from `yfinance` `info` (`trailingPE`) for Weekly/Monthly; P/B and Debt-to-Equity via `yfinance` `info` if available. |\n",
    "|                           | Dividend Yield           | Retrieved from `yfinance` `info` (`dividendYield`) for Monthly model. |\n",
    "| **Macro-Economic Indicators** | Interest Rates       | Retrieved from FRED API (`FEDFUNDS`) series, resampled to weekly/monthly and forward-filled. |\n",
    "|                           | Economic Indicators      | GDP growth rate, unemployment rate, inflation rate retrieved from FRED API for Monthly model. |\n",
    "| **Time-Based Features**   | Day of the Week          | Extracted as 0-4 (Mon-Fri) from `yfinance` date index using `pandas` `dayofweek` for Daily model. |\n",
    "|                           | Seasonality              | Week of the year for Weekly (via `pandas` `isocalendar().week`); Month of the year for Monthly (via `pandas` `month`). |\n",
    "\n",
    "\n",
    "## 3.2 Stock Data\n",
    "\n",
    "We selected 20 stocks across four industries — Technology, Finance, Healthcare, and Energy — for analysis. The stock data spans from November 2020 to December 2023 for the daily and weekly models. For the monthly model, to ensure a sufficient dataset for the train-test split given the longer horizon, we extended the data range from 2010 to 2023.\n",
    "\n",
    "| Technology           | Finance             | Healthcare          | Energy             |\n",
    "|:---------------------|:--------------------|:--------------------|:-------------------|\n",
    "| AAPL (Apple)         | JPM (JPMorgan Chase)| JNJ (Johnson & Johnson) | XOM (ExxonMobil)   |\n",
    "| MSFT (Microsoft)     | BAC (Bank of America)| PFE (Pfizer)       | CVX (Chevron)      |\n",
    "| GOOGL (Alphabet)     | MS (Morgan Stanley)   | MRK (Merck)         | SLB (Schlumberger) |\n",
    "| AMZN (Amazon)        | MS (Goldman Sachs)  | LLY (Eli Lilly)     | COP (ConocoPhillips)|\n",
    "| NVDA (NVIDIA)        | C (Citigroup)       | ABBV (AbbVie)       | EOG (EOG Resources)|\n",
    "\n",
    "\n",
    "## 3.3 τ Value\n",
    "\n",
    "We set τ = 0.01 (1%), a common threshold in finance for daily returns. This balances noise and meaningful moves, as stocks often fluctuate ±1% daily and captures significant shifts without over-classifying noise as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93651ab7",
   "metadata": {},
   "source": [
    "# 4. Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c42e",
   "metadata": {},
   "source": [
    "## 4.1. Daily Model Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2585130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  20 of 20 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded. Shape: (795, 100)\n",
      "DataFrame concatenated. Shape before dropna: (15900, 23)\n",
      "DataFrame after dropna. Shape: (15240, 23)\n",
      "Dataset saved as 'daily_model_data.csv'\n",
      "Price       Date Stock  Y       SMA_5      SMA_10      SMA_20       EMA_5  \\\n",
      "33    2020-12-18  AAPL  1  123.627269  121.940367  119.040793  123.619508   \n",
      "34    2020-12-21  AAPL  1  124.887314  122.377964  119.572650  124.163880   \n",
      "35    2020-12-22  AAPL  0  125.668741  123.110553  120.453219  125.715221   \n",
      "36    2020-12-23  AAPL  0  126.284113  124.007236  121.224389  126.449892   \n",
      "37    2020-12-24  AAPL  1  126.922932  124.859969  122.002884  127.268536   \n",
      "\n",
      "Price      EMA_10      EMA_20  Volatility_20  ...     Volume  Volume_SMA_5  \\\n",
      "33     122.190325  119.879217       0.017857  ...  192541500   124307620.0   \n",
      "34     122.747107  120.390970       0.017629  ...  121251600   132721040.0   \n",
      "35     123.850888  121.193535       0.016426  ...  168904800   135053260.0   \n",
      "36     124.590587  121.834078       0.016705  ...   88223700   133056280.0   \n",
      "37     125.375176  122.507577       0.016705  ...   54930100   125170340.0   \n",
      "\n",
      "Price  Volume_SMA_10  Volume_SMA_20        OBV      MACD  MACD_Signal  \\\n",
      "33       107381680.0    102790785.0  264973200  2.966445     2.721437   \n",
      "34       110835640.0    105173150.0  386224800  3.031643     2.783478   \n",
      "35       119503570.0    107220425.0  555129600  3.332585     2.893299   \n",
      "36       116817020.0    105937900.0  466905900  3.458699     3.006379   \n",
      "37       114178810.0    104859445.0  521836000  3.596793     3.124462   \n",
      "\n",
      "Price   Stoch_%K    ATR_14  Day_of_Week  \n",
      "33     69.487810  2.849619            4  \n",
      "34     85.683817  2.985158            0  \n",
      "35     82.258090  3.203113            1  \n",
      "36     75.806284  3.089439            2  \n",
      "37     82.889303  3.043193            3  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Final Dataset Shape: (15240, 23)\n"
     ]
    }
   ],
   "source": [
    "# Define the 20 stocks\n",
    "stocks = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA',  # Technology\n",
    "    'JPM', 'BAC', 'MS', 'GS', 'C',            # Finance\n",
    "    'JNJ', 'PFE', 'MRK', 'LLY', 'ABBV',       # Healthcare\n",
    "    'XOM', 'CVX', 'SLB', 'COP', 'EOG'         # Energy\n",
    "]\n",
    "\n",
    "# Pull OHLCV data from yfinance\n",
    "start_date = '2020-11-01'\n",
    "end_date = '2023-12-31'\n",
    "try:\n",
    "    data = yf.download(stocks, start=start_date, end=end_date, group_by='ticker')\n",
    "    print(\"Data downloaded. Shape:\", data.shape)\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Downloaded data is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Process each stock\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        if stock not in data.columns.levels[0]:\n",
    "            print(f\"Warning: No data for {stock} - skipping.\")\n",
    "            continue\n",
    "        df = data[stock].copy()\n",
    "        if df.empty or df['Close'].isna().all():\n",
    "            print(f\"Warning: Empty or all-NaN data for {stock} - skipping.\")\n",
    "            continue\n",
    "        df['Stock'] = stock\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        # Compute Daily Returns (R_t+1) and shift for Y\n",
    "        df['Returns'] = df['Close'].pct_change().shift(-1)\n",
    "        df['Y'] = np.where(df['Returns'] > 0.01, 1,\n",
    "                           np.where(df['Returns'] < -0.01, -1, 0))\n",
    "        \n",
    "        # Price-Based Features\n",
    "        df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "        df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "        df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "        df['EMA_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "        df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "        df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "        df['Volatility_20'] = df['Close'].pct_change().rolling(window=20).std()\n",
    "        rsi = RSIIndicator(close=df['Close'], window=14)\n",
    "        df['RSI_14'] = rsi.rsi()\n",
    "        bb = BollingerBands(close=df['Close'], window=20, window_dev=2)\n",
    "        df['BB_Upper'] = bb.bollinger_hband()\n",
    "        df['BB_Lower'] = bb.bollinger_lband()\n",
    "        \n",
    "        # Volume-Based Features\n",
    "        df['Volume_SMA_5'] = df['Volume'].rolling(window=5).mean()\n",
    "        df['Volume_SMA_10'] = df['Volume'].rolling(window=10).mean()\n",
    "        df['Volume_SMA_20'] = df['Volume'].rolling(window=20).mean()\n",
    "        df['OBV'] = np.where(df['Close'] > df['Close'].shift(1), df['Volume'],\n",
    "                             np.where(df['Close'] < df['Close'].shift(1), -df['Volume'], 0)).cumsum()\n",
    "        \n",
    "        # Technical Indicators\n",
    "        macd = MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "        df['MACD'] = macd.macd()\n",
    "        df['MACD_Signal'] = macd.macd_signal()\n",
    "        stoch = StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
    "        df['Stoch_%K'] = stoch.stoch()\n",
    "        atr = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
    "        df['ATR_14'] = atr.average_true_range()\n",
    "        \n",
    "        # Time-Based Features\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "        \n",
    "        # Check for unexpected NaNs after initial 35 days (MACD_Signal needs 26 + 9)\n",
    "        feature_cols = ['SMA_5', 'SMA_10', 'SMA_20', 'EMA_5', 'EMA_10', 'EMA_20',\n",
    "                        'Volatility_20', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume',\n",
    "                        'Volume_SMA_5', 'Volume_SMA_10', 'Volume_SMA_20', 'OBV',\n",
    "                        'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_14', 'Day_of_Week']\n",
    "        if len(df) > 35:  # Enough rows for MACD_Signal\n",
    "            df_after_window = df.iloc[35:]\n",
    "            nan_cols = df_after_window[feature_cols].isna().any()\n",
    "            if nan_cols.any():\n",
    "                print(f\"Warning: Unexpected NaN values in {stock} after initial 35 days in columns: {nan_cols[nan_cols].index.tolist()}\")\n",
    "        \n",
    "        # Select final columns\n",
    "        final_cols = ['Date', 'Stock', 'Y'] + feature_cols\n",
    "        df = df[final_cols]\n",
    "        \n",
    "        all_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stock}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not all_dfs:\n",
    "    raise ValueError(\"No stocks were successfully processed.\")\n",
    "\n",
    "# Concatenate all stock DataFrames\n",
    "try:\n",
    "    daily_model_data = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(\"DataFrame concatenated. Shape before dropna:\", daily_model_data.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error concatenating DataFrames: {e}\")\n",
    "    raise\n",
    "\n",
    "# Drop rows with NaN values\n",
    "daily_model_data = daily_model_data.dropna()\n",
    "print(\"DataFrame after dropna. Shape:\", daily_model_data.shape)\n",
    "\n",
    "# Validate the final DataFrame\n",
    "if daily_model_data.empty:\n",
    "    raise ValueError(\"Final DataFrame is empty after dropping NaNs.\")\n",
    "if daily_model_data['Y'].isna().any():\n",
    "    print(\"Warning: NaN values found in Y column - this should not happen.\")\n",
    "\n",
    "# Save to CSV\n",
    "try:\n",
    "    daily_model_data.to_csv('daily_model_data.csv', index=False)\n",
    "    print(\"Dataset saved as 'daily_model_data.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows and shape\n",
    "print(daily_model_data.head())\n",
    "print(f\"Final Dataset Shape: {daily_model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1feed5d",
   "metadata": {},
   "source": [
    "## 4.2 Weekly Model Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26b6b1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  20 of 20 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded. Shape: (795, 100)\n",
      "DataFrame concatenated. Shape before dropna: (3300, 26)\n",
      "DataFrame after dropna (excluding EPS). Shape: (2320, 26)\n",
      "Dataset saved as 'weekly_model_data.csv'\n",
      "         Date Stock  Y      SMA_10      SMA_20      SMA_50      EMA_10  \\\n",
      "49 2021-10-17  AAPL  1  144.509744  140.311485  130.608782  142.518569   \n",
      "50 2021-10-24  AAPL  0  144.469508  141.439489  131.208499  143.137008   \n",
      "51 2021-10-31  AAPL  1  144.627513  142.550428  131.818868  143.841068   \n",
      "52 2021-11-07  AAPL  0  144.912186  143.592443  132.500126  144.720578   \n",
      "53 2021-11-14  AAPL  1  144.510699  144.441234  133.170679  145.209670   \n",
      "\n",
      "        EMA_20      EMA_50  Volatility_20  ...  Volume_SMA_50         OBV  \\\n",
      "49  139.935909  132.660689       0.022034  ...    447764920.0  3331423600   \n",
      "50  140.505821  133.180661       0.022427  ...    442387310.0  3672114900   \n",
      "51  141.125203  133.722962       0.022417  ...    438450552.0  4064854900   \n",
      "52  141.844553  134.309449       0.022116  ...    437142290.0  4388935200   \n",
      "53  142.374651  134.823219       0.022198  ...    435477808.0  4107135300   \n",
      "\n",
      "        MACD  MACD_Signal   Stoch_%K    ATR_14   EPS  PE_Ratio  Interest_Rate  \\\n",
      "49  3.999752     4.880738  34.597099  6.097882  1.52  33.81399           0.08   \n",
      "50  4.003012     4.705193  54.870996  6.154405  1.52  33.81399           0.08   \n",
      "51  4.046848     4.573524  60.716384  6.188660  1.52  33.81399           0.08   \n",
      "52  4.168218     4.492463  69.672293  6.071164  1.52  33.81399           0.08   \n",
      "53  4.114672     4.416905  62.869431  5.924628  1.52  33.81399           0.08   \n",
      "\n",
      "    Seasonality  \n",
      "49     0.019148  \n",
      "50     0.012675  \n",
      "51    -0.016176  \n",
      "52     0.043287  \n",
      "53     0.025870  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Final Dataset Shape: (2320, 26)\n"
     ]
    }
   ],
   "source": [
    "# Initialize FRED API with your key\n",
    "fred = Fred(api_key='b190dd4fb366b045066de88c061298ca')\n",
    "\n",
    "# Define stocks\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'JPM', 'BAC', 'MS', 'GS', 'C',\n",
    "          'JNJ', 'PFE', 'MRK', 'LLY', 'ABBV', 'XOM', 'CVX', 'SLB', 'COP', 'EOG']\n",
    "\n",
    "# Pull OHLCV data (daily, then resample to weekly)\n",
    "try:\n",
    "    data = yf.download(stocks, start='2020-11-01', end='2023-12-31', group_by='ticker')\n",
    "    print(\"Data downloaded. Shape:\", data.shape)\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Downloaded data is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Fetch interest rates from FRED\n",
    "try:\n",
    "    interest_rates = fred.get_series('FEDFUNDS', start_date='2020-11-01', end='2023-12-31')\n",
    "    interest_rates = interest_rates.resample('W').ffill()\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching interest rates: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize list for DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Process each stock\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        if stock not in data.columns.levels[0]:\n",
    "            print(f\"Warning: No data for {stock} - skipping.\")\n",
    "            continue\n",
    "        df = data[stock].copy()\n",
    "        df['Stock'] = stock\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        # Resample to weekly\n",
    "        df_weekly = df.resample('W', on='Date').agg({'Open': 'first', 'High': 'max', \n",
    "                                                     'Low': 'min', 'Close': 'last', \n",
    "                                                     'Volume': 'sum', 'Stock': 'first'}).reset_index()\n",
    "        \n",
    "        # Returns and Y (weekly)\n",
    "        df_weekly['Returns'] = df_weekly['Close'].pct_change().shift(-1)\n",
    "        df_weekly['Y'] = np.where(df_weekly['Returns'] > 0.01, 1,\n",
    "                                 np.where(df_weekly['Returns'] < -0.01, -1, 0))\n",
    "        \n",
    "        # Price-Based Features\n",
    "        df_weekly['SMA_10'] = df_weekly['Close'].rolling(window=10).mean()\n",
    "        df_weekly['SMA_20'] = df_weekly['Close'].rolling(window=20).mean()\n",
    "        df_weekly['SMA_50'] = df_weekly['Close'].rolling(window=50).mean()\n",
    "        df_weekly['EMA_10'] = df_weekly['Close'].ewm(span=10, adjust=False).mean()\n",
    "        df_weekly['EMA_20'] = df_weekly['Close'].ewm(span=20, adjust=False).mean()\n",
    "        df_weekly['EMA_50'] = df_weekly['Close'].ewm(span=50, adjust=False).mean()\n",
    "        df_weekly['Volatility_20'] = df_weekly['Close'].pct_change().rolling(window=20).std()\n",
    "        \n",
    "        # RSI\n",
    "        df_weekly['RSI_14'] = (df_weekly['Close'].diff().clip(lower=0).rolling(14).mean() /\n",
    "                              df_weekly['Close'].diff().abs().rolling(14).mean() * 100)\n",
    "        \n",
    "        # Bollinger Bands (50-day, 2 std dev)\n",
    "        bb = BollingerBands(close=df_weekly['Close'], window=50, window_dev=2)\n",
    "        df_weekly['BB_Upper'] = bb.bollinger_hband()\n",
    "        df_weekly['BB_Lower'] = bb.bollinger_lband()\n",
    "        \n",
    "        # Volume-Based Features\n",
    "        df_weekly['Volume'] = df_weekly['Volume']\n",
    "        df_weekly['Volume_SMA_10'] = df_weekly['Volume'].rolling(window=10).mean()\n",
    "        df_weekly['Volume_SMA_20'] = df_weekly['Volume'].rolling(window=20).mean()\n",
    "        df_weekly['Volume_SMA_50'] = df_weekly['Volume'].rolling(window=50).mean()\n",
    "        df_weekly['OBV'] = OnBalanceVolumeIndicator(close=df_weekly['Close'], volume=df_weekly['Volume']).on_balance_volume()\n",
    "        \n",
    "        # Technical Indicators\n",
    "        macd = MACD(close=df_weekly['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "        df_weekly['MACD'] = macd.macd()\n",
    "        df_weekly['MACD_Signal'] = macd.macd_signal()\n",
    "        stoch = StochasticOscillator(high=df_weekly['High'], low=df_weekly['Low'], close=df_weekly['Close'], window=14)\n",
    "        df_weekly['Stoch_%K'] = stoch.stoch()\n",
    "        atr = AverageTrueRange(high=df_weekly['High'], low=df_weekly['Low'], close=df_weekly['Close'], window=14)\n",
    "        df_weekly['ATR_14'] = atr.average_true_range()\n",
    "        \n",
    "        # Earnings data (using Reported EPS with nearest quarter)\n",
    "        ticker = yf.Ticker(stock)\n",
    "        try:\n",
    "            earnings_dates = ticker.get_earnings_dates(limit=16)  # Last 4 years (~16 quarters)\n",
    "            if isinstance(earnings_dates, pd.DataFrame) and not earnings_dates.empty:\n",
    "                earnings_dates = earnings_dates.reset_index()\n",
    "                earnings_dates['Date'] = pd.to_datetime(earnings_dates['Earnings Date']).dt.tz_localize(None)\n",
    "                earnings_dates['EPS'] = earnings_dates['Reported EPS'].fillna(0)\n",
    "\n",
    "                # Sort earnings dates for nearest merge\n",
    "                earnings_dates = earnings_dates.sort_values('Date')\n",
    "\n",
    "                # Merge with weekly data using nearest date\n",
    "                df_weekly = pd.merge_asof(df_weekly, earnings_dates[['Date', 'EPS']],\n",
    "                                          on='Date', direction='nearest',\n",
    "                                          suffixes=('', '_earnings'))\n",
    "\n",
    "                # Forward-fill within quarter and handle initial NaNs\n",
    "                df_weekly['Quarter'] = df_weekly['Date'].dt.to_period('Q')\n",
    "                df_weekly['EPS'] = df_weekly.groupby('Quarter')['EPS'].ffill().fillna(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to fetch earnings for {stock}: {e}\")\n",
    "            df_weekly['EPS'] = 0\n",
    "        \n",
    "        # Financial ratios\n",
    "        info = ticker.info\n",
    "        df_weekly['PE_Ratio'] = info.get('trailingPE', np.nan)\n",
    "        \n",
    "        # Interest rates\n",
    "        df_weekly = df_weekly.merge(interest_rates.rename('Interest_Rate'), \n",
    "                                   left_on='Date', right_index=True, how='left')\n",
    "        \n",
    "        # Seasonality\n",
    "        df_weekly['Week_of_Year'] = df_weekly['Date'].dt.isocalendar().week\n",
    "        seasonality = df_weekly.groupby('Week_of_Year')['Returns'].mean().to_dict()\n",
    "        df_weekly['Seasonality'] = df_weekly['Week_of_Year'].map(seasonality)\n",
    "        \n",
    "        # Check for NaNs after initial window (e.g., ATR_14 needs 14 weeks)\n",
    "        feature_cols = ['SMA_10', 'SMA_20', 'SMA_50', 'EMA_10', 'EMA_20', 'EMA_50',\n",
    "                        'Volatility_20', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume',\n",
    "                        'Volume_SMA_10', 'Volume_SMA_20', 'Volume_SMA_50', 'OBV',\n",
    "                        'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_14', 'EPS',\n",
    "                        'PE_Ratio', 'Interest_Rate', 'Seasonality']\n",
    "        if len(df_weekly) > 50:  # Use 50 for BB and MACD\n",
    "            df_after_window = df_weekly.iloc[50:]\n",
    "            nan_cols = df_after_window[feature_cols].isna().any()\n",
    "            if nan_cols.any():\n",
    "                print(f\"Warning: Unexpected NaN values in {stock} after initial 50 weeks in columns: {nan_cols[nan_cols].index.tolist()}\")\n",
    "        \n",
    "        # Select final columns and drop temporary Quarter column\n",
    "        final_cols = ['Date', 'Stock', 'Y'] + feature_cols\n",
    "        df_weekly = df_weekly[final_cols]\n",
    "        \n",
    "        all_dfs.append(df_weekly)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stock}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not all_dfs:\n",
    "    raise ValueError(\"No stocks were successfully processed.\")\n",
    "\n",
    "# Concatenate and clean\n",
    "try:\n",
    "    weekly_model_data = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(\"DataFrame concatenated. Shape before dropna:\", weekly_model_data.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error concatenating DataFrames: {e}\")\n",
    "    raise\n",
    "\n",
    "# Drop rows with NaN values (only for technical indicators, not EPS)\n",
    "weekly_model_data = weekly_model_data.dropna(subset=[col for col in feature_cols if col != 'EPS'])\n",
    "print(\"DataFrame after dropna (excluding EPS). Shape:\", weekly_model_data.shape)\n",
    "\n",
    "# Fill EPS NaNs with 0 (though nearest/quarterly fill should handle this)\n",
    "weekly_model_data['EPS'] = weekly_model_data['EPS'].fillna(0)\n",
    "\n",
    "# Validate\n",
    "if weekly_model_data.empty:\n",
    "    raise ValueError(\"Final DataFrame is empty after dropping NaNs.\")\n",
    "if weekly_model_data['Y'].isna().any():\n",
    "    print(\"Warning: NaN values found in Y column - this should not happen.\")\n",
    "\n",
    "# Save to CSV\n",
    "try:\n",
    "    weekly_model_data.to_csv('weekly_model_data.csv', index=False)\n",
    "    print(\"Dataset saved as 'weekly_model_data.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display\n",
    "print(weekly_model_data.head())\n",
    "print(f\"Final Dataset Shape: {weekly_model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712180d",
   "metadata": {},
   "source": [
    "## 4.3 Monthly Model Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62cee7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1907019150.py, line 176)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[74], line 176\u001b[1;36m\u001b[0m\n\u001b[1;33m    monthly_model_data = monthly_model_data.dropna(subset=[col for col in feature_cols if col not in ['EPS', 'Interest_Rate', 'GDP_Growth', \\\\\u001b[0m\n\u001b[1;37m                                                                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Define stocks\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'JPM', 'BAC', 'MS', 'GS', 'C',\n",
    "          'JNJ', 'PFE', 'MRK', 'LLY', 'ABBV', 'XOM', 'CVX', 'SLB', 'COP', 'EOG']\n",
    "\n",
    "# Initialize FRED API with your key\n",
    "fred = Fred(api_key='b190dd4fb366b045066de88c061298ca')\n",
    "\n",
    "# Fetch economic indicators (adjusted start date to 2010)\n",
    "try:\n",
    "    gdp_growth = fred.get_series('GDP', start_date='2010-01-01', end='2023-12-31').resample('M').ffill()\n",
    "    unemployment_rate = fred.get_series('UNRATE', start_date='2010-01-01', end='2023-12-31').resample('M').ffill()\n",
    "    inflation_rate = fred.get_series('CPIAUCSL', start_date='2010-01-01', end='2023-12-31').resample('M').ffill()\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching economic indicators: {e}\")\n",
    "    raise\n",
    "\n",
    "# Combine economic indicators into a single DataFrame\n",
    "economic_indicators = pd.DataFrame({\n",
    "    'GDP_Growth': gdp_growth,\n",
    "    'Unemployment_Rate': unemployment_rate,\n",
    "    'Inflation_Rate': inflation_rate\n",
    "})\n",
    "\n",
    "# Fetch interest rates\n",
    "try:\n",
    "    interest_rates = fred.get_series('FEDFUNDS', start_date='2010-01-01', end='2023-12-31').resample('M').ffill()\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching interest rates: {e}\")\n",
    "    raise\n",
    "\n",
    "# Pull OHLCV data (daily, then resample to monthly)\n",
    "try:\n",
    "    data = yf.download(stocks, start='2010-01-01', end='2023-12-31', group_by='ticker')\n",
    "    print(\"Data downloaded. Shape:\", data.shape)\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Downloaded data is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize list for DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Process each stock\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        if stock not in data.columns.levels[0]:\n",
    "            print(f\"Warning: No data for {stock} - skipping.\")\n",
    "            continue\n",
    "        df = data[stock].copy()\n",
    "        df['Stock'] = stock\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        # Resample to monthly\n",
    "        df_monthly = df.resample('M', on='Date').agg({'Open': 'first', 'High': 'max', \n",
    "                                                      'Low': 'min', 'Close': 'last', \n",
    "                                                      'Volume': 'sum', 'Stock': 'first'}).reset_index()\n",
    "        \n",
    "        # Ensure enough data points\n",
    "        if len(df_monthly) < 20:\n",
    "            print(f\"Warning: Not enough data for {stock} - skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Returns and Y (monthly)\n",
    "        df_monthly['Returns'] = df_monthly['Close'].pct_change().shift(-1)\n",
    "        df_monthly['Y'] = np.where(df_monthly['Returns'] > 0.01, 1,\n",
    "                                   np.where(df_monthly['Returns'] < -0.01, -1, 0))\n",
    "        \n",
    "        # Price-Based Features\n",
    "        df_monthly['SMA_20'] = df_monthly['Close'].rolling(window=20).mean()\n",
    "        df_monthly['SMA_50'] = df_monthly['Close'].rolling(window=50).mean()\n",
    "        df_monthly['SMA_100'] = df_monthly['Close'].rolling(window=100).mean()\n",
    "        df_monthly['EMA_20'] = df_monthly['Close'].ewm(span=20, adjust=False).mean()\n",
    "        df_monthly['EMA_50'] = df_monthly['Close'].ewm(span=50, adjust=False).mean()\n",
    "        df_monthly['EMA_100'] = df_monthly['Close'].ewm(span=100, adjust=False).mean()\n",
    "        df_monthly['Volatility_50'] = df_monthly['Close'].pct_change().rolling(window=50).std()\n",
    "        \n",
    "        # RSI\n",
    "        df_monthly['RSI_14'] = (df_monthly['Close'].diff().clip(lower=0).rolling(14).mean() /\n",
    "                                df_monthly['Close'].diff().abs().rolling(14).mean() * 100)\n",
    "        \n",
    "        # Bollinger Bands (100-day, 2 std dev)\n",
    "        bb = BollingerBands(close=df_monthly['Close'], window=100, window_dev=2)\n",
    "        df_monthly['BB_Upper'] = bb.bollinger_hband()\n",
    "        df_monthly['BB_Lower'] = bb.bollinger_lband()\n",
    "        \n",
    "        # Volume-Based Features\n",
    "        df_monthly['Volume'] = df_monthly['Volume']\n",
    "        df_monthly['Volume_SMA_20'] = df_monthly['Volume'].rolling(window=20).mean()\n",
    "        df_monthly['Volume_SMA_50'] = df_monthly['Volume'].rolling(window=50).mean()\n",
    "        df_monthly['Volume_SMA_100'] = df_monthly['Volume'].rolling(window=100).mean()\n",
    "        df_monthly['OBV'] = OnBalanceVolumeIndicator(close=df_monthly['Close'], volume=df_monthly['Volume']).on_balance_volume()\n",
    "        \n",
    "        # Technical Indicators\n",
    "        macd = MACD(close=df_monthly['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "        df_monthly['MACD'] = macd.macd()\n",
    "        df_monthly['MACD_Signal'] = macd.macd_signal()\n",
    "        stoch = StochasticOscillator(high=df_monthly['High'], low=df_monthly['Low'], close=df_monthly['Close'], window=14)\n",
    "        df_monthly['Stoch_%K'] = stoch.stoch()\n",
    "        atr = AverageTrueRange(high=df_monthly['High'], low=df_monthly['Low'], close=df_monthly['Close'], window=50)\n",
    "        df_monthly['ATR_50'] = atr.average_true_range()\n",
    "        \n",
    "        # Earnings data (using Reported EPS with nearest quarter)\n",
    "        ticker = yf.Ticker(stock)\n",
    "        try:\n",
    "            earnings_dates = ticker.get_earnings_dates(limit=168)  # ~14 years (~168 quarters)\n",
    "            if isinstance(earnings_dates, pd.DataFrame) and not earnings_dates.empty:\n",
    "                earnings_dates = earnings_dates.reset_index()\n",
    "                earnings_dates['Date'] = pd.to_datetime(earnings_dates['Earnings Date']).dt.tz_localize(None)\n",
    "                earnings_dates['EPS'] = earnings_dates['Reported EPS'].fillna(0)\n",
    "\n",
    "                # Sort earnings dates for nearest merge\n",
    "                earnings_dates = earnings_dates.sort_values('Date')\n",
    "\n",
    "                # Merge with monthly data using nearest date\n",
    "                df_monthly = pd.merge_asof(df_monthly, earnings_dates[['Date', 'EPS']],\n",
    "                                           on='Date', direction='nearest',\n",
    "                                           suffixes=('', '_earnings'))\n",
    "\n",
    "                # Forward-fill within quarter and handle initial NaNs\n",
    "                df_monthly['Quarter'] = df_monthly['Date'].dt.to_period('Q')\n",
    "                df_monthly['EPS'] = df_monthly.groupby('Quarter')['EPS'].ffill().fillna(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to fetch earnings for {stock}: {e}\")\n",
    "            df_monthly['EPS'] = 0\n",
    "        \n",
    "        # Financial ratios and dividend yield\n",
    "        info = ticker.info\n",
    "        df_monthly['PE_Ratio'] = info.get('trailingPE', np.nan)\n",
    "        df_monthly['Dividend_Yield'] = dividend_yields.get(stock, 0)\n",
    "        \n",
    "        # Interest rates and economic indicators\n",
    "        df_monthly = df_monthly.merge(interest_rates.rename('Interest_Rate'), \n",
    "                                      left_on='Date', right_index=True, how='left')\n",
    "        df_monthly = df_monthly.merge(economic_indicators, left_on='Date', right_index=True, how='left')\n",
    "        \n",
    "        # Seasonality\n",
    "        df_monthly['Month_of_Year'] = df_monthly['Date'].dt.month\n",
    "        seasonality = df_monthly.groupby('Month_of_Year')['Returns'].mean().to_dict()\n",
    "        df_monthly['Seasonality'] = df_monthly['Month_of_Year'].map(seasonality).fillna(0)\n",
    "        \n",
    "        # Check for NaNs after initial window (e.g., 100 for BB)\n",
    "        feature_cols = ['SMA_20', 'SMA_50', 'SMA_100', 'EMA_20', 'EMA_50', 'EMA_100',\n",
    "                        'Volatility_50', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume',\n",
    "                        'Volume_SMA_20', 'Volume_SMA_50', 'Volume_SMA_100', 'OBV',\n",
    "                        'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_50', 'EPS',\n",
    "                        'PE_Ratio', 'Dividend_Yield', 'Interest_Rate', 'GDP_Growth',\n",
    "                        'Unemployment_Rate', 'Inflation_Rate', 'Seasonality']\n",
    "        if len(df_monthly) > 100:  # Use 100 for BB\n",
    "            df_after_window = df_monthly.iloc[100:]\n",
    "            nan_cols = df_after_window[feature_cols].isna().any()\n",
    "            if nan_cols.any():\n",
    "                print(f\"Warning: Unexpected NaN values in {stock} after initial 100 months in columns: {nan_cols[nan_cols].index.tolist()}\")\n",
    "        \n",
    "        # Select final columns and drop temporary Quarter column\n",
    "        final_cols = ['Date', 'Stock', 'Y'] + feature_cols\n",
    "        df_monthly = df_monthly[final_cols]\n",
    "        \n",
    "        all_dfs.append(df_monthly)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stock}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not all_dfs:\n",
    "    raise ValueError(\"No stocks were successfully processed.\")\n",
    "\n",
    "# Concatenate and clean\n",
    "try:\n",
    "    monthly_model_data = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(\"DataFrame concatenated. Shape before dropna:\", monthly_model_data.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error concatenating DataFrames: {e}\")\n",
    "    raise\n",
    "\n",
    "# Drop rows with NaN values (only for technical indicators, not EPS or economic indicators)\n",
    "monthly_model_data = monthly_model_data.dropna(subset=[col for col in feature_cols if col not in ['EPS', 'Interest_Rate', 'GDP_Growth', 'Unemployment_Rate', 'Inflation_Rate']])\n",
    "print(\"DataFrame after dropna (excluding EPS and economic indicators). Shape:\", monthly_model_data.shape)\n",
    "\n",
    "# Fill EPS and economic indicator NaNs with 0 or last valid value\n",
    "monthly_model_data['EPS'] = monthly_model_data['EPS'].fillna(0)\n",
    "monthly_model_data[['Interest_Rate', 'GDP_Growth', 'Unemployment_Rate', 'Inflation_Rate']] = monthly_model_data[['Interest_Rate', 'GDP_Growth', 'Unemployment_Rate', 'Inflation_Rate']].fillna(method='ffill').fillna(0)\n",
    "\n",
    "# Validate\n",
    "if monthly_model_data.empty:\n",
    "    raise ValueError(\"Final DataFrame is empty after dropping NaNs.\")\n",
    "if monthly_model_data['Y'].isna().any():\n",
    "    print(\"Warning: NaN values found in Y column - this should not happen.\")\n",
    "\n",
    "# Save to CSV\n",
    "try:\n",
    "    monthly_model_data.to_csv('monthly_model_data.csv', index=False)\n",
    "    print(\"Dataset saved as 'monthly_model_data.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display\n",
    "print(monthly_model_data.head())\n",
    "print(f\"Final Dataset Shape: {monthly_model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cd21f",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95946eb",
   "metadata": {},
   "source": [
    "## 5.1 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4ea28",
   "metadata": {},
   "source": [
    "### 5.1.1 Daily Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5413321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (15240, 23)\n",
      "Train shape: (12180, 23)\n",
      "Test shape: (3060, 23)\n",
      "Train date range: 2020-12-18 00:00:00 to 2023-05-22 00:00:00\n",
      "Test date range: 2023-05-23 00:00:00 to 2023-12-29 00:00:00\n",
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.5715686274509804\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.03      0.05       619\n",
      "           0       0.59      0.96      0.73      1763\n",
      "           1       0.35      0.06      0.11       678\n",
      "\n",
      "    accuracy                           0.57      3060\n",
      "   macro avg       0.40      0.35      0.30      3060\n",
      "weighted avg       0.47      0.57      0.45      3060\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted -1  Predicted 0  Predicted 1\n",
      "Actual -1            17          570           32\n",
      "Actual 0             27         1688           48\n",
      "Actual 1             21          613           44\n",
      "ROC-AUC (one-vs-rest): 0.5666569178800639\n",
      "\n",
      "Dummy Classifier Baseline:\n",
      "Accuracy: 0.5761437908496732\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with error handling\n",
    "try:\n",
    "    data = pd.read_csv('daily_model_data.csv')\n",
    "    print(\"Data loaded. Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'daily_model_data.csv' not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ensure Date is datetime\n",
    "try:\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "except Exception as e:\n",
    "    print(f\"Error converting Date to datetime: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = ['SMA_5', 'SMA_10', 'SMA_20', 'EMA_5', 'EMA_10', 'EMA_20', \n",
    "                'Volatility_20', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume', \n",
    "                'Volume_SMA_5', 'Volume_SMA_10', 'Volume_SMA_20', 'OBV', \n",
    "                'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_14', 'Day_of_Week']\n",
    "\n",
    "# Time-based 80/20 split\n",
    "try:\n",
    "    unique_dates = data['Date'].unique()\n",
    "    if len(unique_dates) < 2:\n",
    "        raise ValueError(\"Not enough unique dates for splitting.\")\n",
    "    train_days = int(len(unique_dates) * 0.8)  # 80% of unique dates\n",
    "    train_dates = unique_dates[:train_days]\n",
    "    test_dates = unique_dates[train_days:]\n",
    "\n",
    "    train_data = data[data['Date'].isin(train_dates)]\n",
    "    test_data = data[data['Date'].isin(test_dates)]\n",
    "    if train_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Train or test split resulted in empty dataset.\")\n",
    "    print(\"Train shape:\", train_data.shape)\n",
    "    print(\"Test shape:\", test_data.shape)\n",
    "    print(\"Train date range:\", train_data['Date'].min(), \"to\", train_data['Date'].max())\n",
    "    print(\"Test date range:\", test_data['Date'].min(), \"to\", test_data['Date'].max())\n",
    "except Exception as e:\n",
    "    print(f\"Error during train-test split: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare X and y\n",
    "try:\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['Y']\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data['Y']\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "        raise ValueError(\"Features or target data is empty.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing column in data: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing X and y: {e}\")\n",
    "    raise\n",
    "\n",
    "# Scale features\n",
    "try:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "except Exception as e:\n",
    "    print(f\"Error scaling features: {e}\")\n",
    "    raise\n",
    "\n",
    "# Train logistic regression\n",
    "try:\n",
    "    model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "except Exception as e:\n",
    "    print(f\"Error training logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Predict on test data\n",
    "try:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error predicting with logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Baseline: Dummy classifier (most frequent)\n",
    "try:\n",
    "    dummy = DummyClassifier(strategy='most_frequent')\n",
    "    dummy.fit(X_train_scaled, y_train)\n",
    "    y_dummy_pred = dummy.predict(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error with dummy classifier: {e}\")\n",
    "    raise\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    print(\"\\nLogistic Regression Performance:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['-1', '0', '1']))\n",
    "    \n",
    "    # Improved Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=['Actual -1', 'Actual 0', 'Actual 1'], \n",
    "                        columns=['Predicted -1', 'Predicted 0', 'Predicted 1'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(\"ROC-AUC (one-vs-rest):\", roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
    "\n",
    "    print(\"\\nDummy Classifier Baseline:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_dummy_pred))\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating performance: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1b4c9",
   "metadata": {},
   "source": [
    "### 5.1.2 Weekly Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a024a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (2320, 26)\n",
      "Train shape: (1840, 26)\n",
      "Test shape: (480, 26)\n",
      "Train date range: 2021-10-17 00:00:00 to 2023-07-16 00:00:00\n",
      "Test date range: 2023-07-23 00:00:00 to 2023-12-31 00:00:00\n",
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.4625\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.51      0.50       162\n",
      "           0       0.35      0.05      0.09       140\n",
      "           1       0.46      0.75      0.57       178\n",
      "\n",
      "    accuracy                           0.46       480\n",
      "   macro avg       0.43      0.43      0.38       480\n",
      "weighted avg       0.44      0.46      0.40       480\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted -1  Predicted 0  Predicted 1\n",
      "Actual -1            82            7           73\n",
      "Actual 0             48            7           85\n",
      "Actual 1             39            6          133\n",
      "ROC-AUC (one-vs-rest): 0.6375470189870948\n",
      "\n",
      "Dummy Classifier Baseline:\n",
      "Accuracy: 0.37083333333333335\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with error handling\n",
    "try:\n",
    "    data = pd.read_csv('weekly_model_data.csv')\n",
    "    print(\"Data loaded. Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'weekly_model_data.csv' not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ensure Date is datetime\n",
    "try:\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "except Exception as e:\n",
    "    print(f\"Error converting Date to datetime: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define feature columns (excluding 'Date', 'Stock', and 'Y')\n",
    "feature_cols = ['SMA_10', 'SMA_20', 'SMA_50', 'EMA_10', 'EMA_20', 'EMA_50',\n",
    "                'Volatility_20', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume',\n",
    "                'Volume_SMA_10', 'Volume_SMA_20', 'Volume_SMA_50', 'OBV',\n",
    "                'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_14', 'EPS',\n",
    "                'PE_Ratio', 'Interest_Rate', 'Seasonality']\n",
    "\n",
    "# Time-based 80/20 split\n",
    "try:\n",
    "    unique_dates = data['Date'].unique()\n",
    "    if len(unique_dates) < 2:\n",
    "        raise ValueError(\"Not enough unique dates for splitting.\")\n",
    "    train_days = int(len(unique_dates) * 0.8)  # 80% of unique dates\n",
    "    train_dates = unique_dates[:train_days]\n",
    "    test_dates = unique_dates[train_days:]\n",
    "\n",
    "    train_data = data[data['Date'].isin(train_dates)]\n",
    "    test_data = data[data['Date'].isin(test_dates)]\n",
    "    if train_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Train or test split resulted in empty dataset.\")\n",
    "    print(\"Train shape:\", train_data.shape)\n",
    "    print(\"Test shape:\", test_data.shape)\n",
    "    print(\"Train date range:\", train_data['Date'].min(), \"to\", train_data['Date'].max())\n",
    "    print(\"Test date range:\", test_data['Date'].min(), \"to\", test_data['Date'].max())\n",
    "except Exception as e:\n",
    "    print(f\"Error during train-test split: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare X and y\n",
    "try:\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['Y']\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data['Y']\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "        raise ValueError(\"Features or target data is empty.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing column in data: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing X and y: {e}\")\n",
    "    raise\n",
    "\n",
    "# Scale features\n",
    "try:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "except Exception as e:\n",
    "    print(f\"Error scaling features: {e}\")\n",
    "    raise\n",
    "\n",
    "# Train logistic regression\n",
    "try:\n",
    "    model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "except Exception as e:\n",
    "    print(f\"Error training logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Predict on test data\n",
    "try:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error predicting with logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Baseline: Dummy classifier (most frequent)\n",
    "try:\n",
    "    dummy = DummyClassifier(strategy='most_frequent')\n",
    "    dummy.fit(X_train_scaled, y_train)\n",
    "    y_dummy_pred = dummy.predict(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error with dummy classifier: {e}\")\n",
    "    raise\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    print(\"\\nLogistic Regression Performance:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['-1', '0', '1']))\n",
    "    \n",
    "    # Improved Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=['Actual -1', 'Actual 0', 'Actual 1'], \n",
    "                        columns=['Predicted -1', 'Predicted 0', 'Predicted 1'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(\"ROC-AUC (one-vs-rest):\", roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
    "\n",
    "    print(\"\\nDummy Classifier Baseline:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_dummy_pred))\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating performance: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33a015",
   "metadata": {},
   "source": [
    "### 5.1.3 Monthly Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf8fcc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (1344, 30)\n",
      "Train shape: (1064, 30)\n",
      "Test shape: (280, 30)\n",
      "Train date range: 2018-04-30 00:00:00 to 2022-10-31 00:00:00\n",
      "Test date range: 2022-11-30 00:00:00 to 2023-12-31 00:00:00\n",
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.48928571428571427\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.27      0.37       107\n",
      "           0       0.00      0.00      0.00        48\n",
      "           1       0.47      0.86      0.61       125\n",
      "\n",
      "    accuracy                           0.49       280\n",
      "   macro avg       0.35      0.38      0.33       280\n",
      "weighted avg       0.43      0.49      0.41       280\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted -1  Predicted 0  Predicted 1\n",
      "Actual -1            29            0           78\n",
      "Actual 0              3            0           45\n",
      "Actual 1             17            0          108\n",
      "ROC-AUC (one-vs-rest): 0.5933885711783634\n",
      "\n",
      "Dummy Classifier Baseline:\n",
      "Accuracy: 0.44642857142857145\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with error handling\n",
    "try:\n",
    "    data = pd.read_csv('monthly_model_data.csv')\n",
    "    print(\"Data loaded. Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'monthly_model_data.csv' not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ensure Date is datetime\n",
    "try:\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "except Exception as e:\n",
    "    print(f\"Error converting Date to datetime: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define feature columns (based on the corrected monthly_model_data.csv)\n",
    "feature_cols = ['SMA_20', 'SMA_50', 'SMA_100', 'EMA_20', 'EMA_50', 'EMA_100',\n",
    "                'Volatility_50', 'RSI_14', 'BB_Upper', 'BB_Lower', 'Volume',\n",
    "                'Volume_SMA_20', 'Volume_SMA_50', 'Volume_SMA_100', 'OBV',\n",
    "                'MACD', 'MACD_Signal', 'Stoch_%K', 'ATR_50', 'EPS',\n",
    "                'PE_Ratio', 'Dividend_Yield', 'Interest_Rate', 'GDP_Growth',\n",
    "                'Unemployment_Rate', 'Inflation_Rate', 'Seasonality']\n",
    "\n",
    "# Time-based 80/20 split\n",
    "try:\n",
    "    unique_dates = data['Date'].unique()\n",
    "    if len(unique_dates) < 2:\n",
    "        raise ValueError(\"Not enough unique dates for splitting.\")\n",
    "    train_days = int(len(unique_dates) * 0.8)  # 80% of unique dates\n",
    "    train_dates = unique_dates[:train_days]\n",
    "    test_dates = unique_dates[train_days:]\n",
    "\n",
    "    train_data = data[data['Date'].isin(train_dates)]\n",
    "    test_data = data[data['Date'].isin(test_dates)]\n",
    "    if train_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Train or test split resulted in empty dataset.\")\n",
    "    print(\"Train shape:\", train_data.shape)\n",
    "    print(\"Test shape:\", test_data.shape)\n",
    "    print(\"Train date range:\", train_data['Date'].min(), \"to\", train_data['Date'].max())\n",
    "    print(\"Test date range:\", test_data['Date'].min(), \"to\", test_data['Date'].max())\n",
    "except Exception as e:\n",
    "    print(f\"Error during train-test split: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare X and y\n",
    "try:\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['Y']\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data['Y']\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "        raise ValueError(\"Features or target data is empty.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing column in data: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing X and y: {e}\")\n",
    "    raise\n",
    "\n",
    "# Scale features\n",
    "try:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "except Exception as e:\n",
    "    print(f\"Error scaling features: {e}\")\n",
    "    raise\n",
    "\n",
    "# Train logistic regression\n",
    "try:\n",
    "    model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "except Exception as e:\n",
    "    print(f\"Error training logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Predict on test data\n",
    "try:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error predicting with logistic regression: {e}\")\n",
    "    raise\n",
    "\n",
    "# Baseline: Dummy classifier (most frequent)\n",
    "try:\n",
    "    dummy = DummyClassifier(strategy='most_frequent')\n",
    "    dummy.fit(X_train_scaled, y_train)\n",
    "    y_dummy_pred = dummy.predict(X_test_scaled)\n",
    "except Exception as e:\n",
    "    print(f\"Error with dummy classifier: {e}\")\n",
    "    raise\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    print(\"\\nLogistic Regression Performance:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['-1', '0', '1']))\n",
    "    \n",
    "    # Improved Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=['Actual -1', 'Actual 0', 'Actual 1'], \n",
    "                        columns=['Predicted -1', 'Predicted 0', 'Predicted 1'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(\"ROC-AUC (one-vs-rest):\", roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
    "\n",
    "    print(\"\\nDummy Classifier Baseline:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_dummy_pred))\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating performance: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a43ceb",
   "metadata": {},
   "source": [
    "## 5.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1beb92b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width:100%; border-collapse: collapse;\">\n",
       "    <tr style=\"border-bottom: 1px solid black;\">\n",
       "        <th style=\"padding: 8px;\"></th>\n",
       "        <th style=\"padding: 8px;\">Accuracy</th>\n",
       "        <th style=\"padding: 8px;\">ROC-AUC</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">Precision</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">Recall</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">F1-Score</th>\n",
       "        <th style=\"padding: 8px;\">Dummy Accuracy</th>\n",
       "    </tr>\n",
       "    <tr style=\"border-bottom: 2px solid black;\">\n",
       "        <th style=\"padding: 8px;\"></th>\n",
       "        <th style=\"padding: 8px;\"></th>\n",
       "        <th style=\"padding: 8px;\"></th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
       "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
       "        <th style=\"padding: 8px;\"></th>\n",
       "    </tr>\n",
       "<tr><td style='padding: 8px; font-weight: bold;'>Daily</td><td style='padding: 8px; text-align: center;'>0.571569</td><td style='padding: 8px; text-align: center;'>0.566657</td><td style='padding: 8px; text-align: center;'>0.26</td><td style='padding: 8px; text-align: center;'>0.59</td><td style='padding: 8px; text-align: center;'>0.35</td><td style='padding: 8px; text-align: center;'>0.03</td><td style='padding: 8px; text-align: center;'>0.96</td><td style='padding: 8px; text-align: center;'>0.06</td><td style='padding: 8px; text-align: center;'>0.05</td><td style='padding: 8px; text-align: center;'>0.73</td><td style='padding: 8px; text-align: center;'>0.11</td><td style='padding: 8px; text-align: center;'>0.576144</td></tr><tr><td style='padding: 8px; font-weight: bold;'>Weekly</td><td style='padding: 8px; text-align: center;'>0.4625</td><td style='padding: 8px; text-align: center;'>0.637595</td><td style='padding: 8px; text-align: center;'>0.49</td><td style='padding: 8px; text-align: center;'>0.35</td><td style='padding: 8px; text-align: center;'>0.46</td><td style='padding: 8px; text-align: center;'>0.51</td><td style='padding: 8px; text-align: center;'>0.05</td><td style='padding: 8px; text-align: center;'>0.75</td><td style='padding: 8px; text-align: center;'>0.5</td><td style='padding: 8px; text-align: center;'>0.09</td><td style='padding: 8px; text-align: center;'>0.57</td><td style='padding: 8px; text-align: center;'>0.370833</td></tr><tr><td style='padding: 8px; font-weight: bold;'>Monthly</td><td style='padding: 8px; text-align: center;'>0.489286</td><td style='padding: 8px; text-align: center;'>0.593389</td><td style='padding: 8px; text-align: center;'>0.59</td><td style='padding: 8px; text-align: center;'>0.0</td><td style='padding: 8px; text-align: center;'>0.47</td><td style='padding: 8px; text-align: center;'>0.27</td><td style='padding: 8px; text-align: center;'>0.0</td><td style='padding: 8px; text-align: center;'>0.86</td><td style='padding: 8px; text-align: center;'>0.37</td><td style='padding: 8px; text-align: center;'>0.0</td><td style='padding: 8px; text-align: center;'>0.61</td><td style='padding: 8px; text-align: center;'>0.446429</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with the data\n",
    "data = {\n",
    "    '': ['Daily', 'Weekly', 'Monthly'],\n",
    "    'Accuracy': [0.571569, 0.4625, 0.489286],\n",
    "    'ROC-AUC': [0.566657, 0.637595, 0.593389],\n",
    "    'Precision -1': [0.26, 0.49, 0.59],\n",
    "    'Precision 0': [0.59, 0.35, 0.00],\n",
    "    'Precision 1': [0.35, 0.46, 0.47],\n",
    "    'Recall -1': [0.03, 0.51, 0.27],\n",
    "    'Recall 0': [0.96, 0.05, 0.00],\n",
    "    'Recall 1': [0.06, 0.75, 0.86],\n",
    "    'F1-Score -1': [0.05, 0.50, 0.37],\n",
    "    'F1-Score 0': [0.73, 0.09, 0.00],\n",
    "    'F1-Score 1': [0.11, 0.57, 0.61],\n",
    "    'Dummy Accuracy': [0.576144, 0.370833, 0.446429]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame with merged headers and custom styling\n",
    "html = \"\"\"\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr style=\"border-bottom: 1px solid black;\">\n",
    "        <th style=\"padding: 8px;\"></th>\n",
    "        <th style=\"padding: 8px;\">Accuracy</th>\n",
    "        <th style=\"padding: 8px;\">ROC-AUC</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">Precision</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">Recall</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\" colspan=\"3\">F1-Score</th>\n",
    "        <th style=\"padding: 8px;\">Dummy Accuracy</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 2px solid black;\">\n",
    "        <th style=\"padding: 8px;\"></th>\n",
    "        <th style=\"padding: 8px;\"></th>\n",
    "        <th style=\"padding: 8px;\"></th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">-1</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">0</th>\n",
    "        <th style=\"padding: 8px; text-align: center;\">1</th>\n",
    "        <th style=\"padding: 8px;\"></th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    html += \"<tr>\"\n",
    "    for col in df.columns:\n",
    "        if col == '':\n",
    "            html += f\"<td style='padding: 8px; font-weight: bold;'>{row[col]}</td>\"\n",
    "        else:\n",
    "            html += f\"<td style='padding: 8px; text-align: center;'>{row[col]}</td>\"\n",
    "    html += \"</tr>\"\n",
    "\n",
    "html += \"</table>\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca26c",
   "metadata": {},
   "source": [
    "As shown in the above table, we observed the following:\n",
    "\n",
    "- **Accuracy**: Accuracy is the ratio of correctly predicted instances to the total instances. It measures how often the model is correct. \n",
    "\n",
    "    - The daily model has the highest accuracy (0.571569), followed by the monthly (0.489286) and weekly (0.4625) models. However, the daily model's accuracy is lower than the dummy accuracy (0.576144), indicating it performs worse than random guessing. \n",
    "\n",
    "    - The weekly and monthly models have accuracies higher than their respective dummy accuracies, suggesting they perform better than random guessing but are still relatively low.\n",
    "\n",
    "---\n",
    "\n",
    "- **ROC-AUC**: ROC-AUC (Receiver Operating Characteristic - Area Under Curve) is a performance measurement for classification problems at various threshold settings. A higher AUC value indicates a better performing model that can distinguish between positive and negative classes effectively. \n",
    "\n",
    "    - The weekly model has the highest ROC-AUC (0.637595), indicating it has the best balance between sensitivity and specificity. The monthly model is next (0.593389), followed by the daily model (0.566657).\n",
    "\n",
    "---\n",
    "\n",
    "- **Precision**: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It indicates how many of the predicted positives are actual positives. High precision indicates that the model has a low false positive rate.\n",
    "\n",
    "    - For class -1, the monthly model performs best (0.59). \n",
    "\n",
    "    - For class 0, the daily model performs best (0.59). \n",
    "\n",
    "    - For class 1, the monthly model performs best (0.47). \n",
    "\n",
    "---\n",
    "\n",
    "- **Recall**: Recall is the ratio of correctly predicted positive observations to all the observations in the actual class. It indicates how many of the actual positives are captured by the model. High recall indicates that the model has a low false negative rate.\n",
    "\n",
    "    - For class -1, the weekly model performs best (0.51). \n",
    "\n",
    "    - For class 0, the daily model performs best (0.96). \n",
    "\n",
    "    - For class 1, the monthly model performs best (0.86). \n",
    "\n",
    "---\n",
    "\n",
    "- **F1-Score**: The F1-Score is the weighted average of Precision and Recall. It considers both false positives and false negatives. A high F1-Score indicates a balance between precision and recall.\n",
    "\n",
    "    - For class -1, the weekly model performs best (0.50). \n",
    "\n",
    "    - For class 0, the daily model performs best (0.73). \n",
    "\n",
    "    - For class 1, the monthly model performs best (0.61). \n",
    "\n",
    "---\n",
    "\n",
    "- **Dummy Accuracy**: Dummy accuracy is the accuracy of a simple model that makes predictions based on the most frequent class or random guessing. It serves as a baseline to compare the performance of the actual model. \n",
    "\n",
    "    - The daily model's accuracy (0.571569) is lower than the dummy accuracy (0.576144), which indicates that it performs worse than random guessing. \n",
    "\n",
    "    - The weekly model's accuracy (0.4625) is higher than its dummy accuracy (0.370833), which suggests that it performs better than random guessing. \n",
    "\n",
    "    - The monthly model's accuracy (0.489286) is higher than its dummy accuracy (0.446429), which indicates that it performs better than random guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d52bbb",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bc9ae",
   "metadata": {},
   "source": [
    "The overall performance of all three models is not very impressive. The daily model's accuracy is even worse than the dummy accuracy, indicating it performs worse than random guessing. The weekly and monthly models have accuracies higher than their respective dummy accuracies, but they are still relatively low, suggesting that these models are not highly reliable.\n",
    "\n",
    "The weekly model shows the best balance between sensitivity and specificity as indicated by the highest ROC-AUC. However, the overall performance metrics suggest that none of the models are performing exceptionally well.\n",
    "\n",
    "In summary, while the weekly model appears to provide the most balanced performance across different metrics, the overall performance of all models indicates that there is significant room for improvement in the classification models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
